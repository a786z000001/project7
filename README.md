HalluGuard â€” Research-Style Factual Verification Engine
ğŸš€ Overview

HalluGuard is a hybrid, research-inspired factual verification engine designed to detect and score hallucination risk in natural language claims.

Unlike naive LLM-based fact checking systems, HalluGuard combines:

Transformer-based triple extraction (REBEL)

Structured knowledge graph grounding (Wikidata)

Property-aware deterministic reasoning

NLI-based semantic verification

GPT fallback for uncertain cases

Calibrated hallucination scoring

The system mimics modern research architectures used in FEVER-style fact verification systems.

ğŸ— System Architecture
User Input
    â†“
Claim Classification
    â†“
Triple Extraction (REBEL Transformer)
    â†“
Subjectâ€“Relationâ€“Object Representation
    â†“
Property Mapping â†’ Wikidata Query
    â†“
Structured Deterministic Verification
    â†“
NLI Fallback (BART-MNLI)
    â†“
LLM Fallback (OpenAI GPT)
    â†“
Hallucination Risk Scoring
ğŸ”¬ Core Components
1ï¸âƒ£ Transformer-Based Triple Extraction

Model: Babelscape/rebel-large

Extracts structured triples:

Argentina won the FIFA World Cup in 2022
â†’
{
  subject: Argentina
  relation: winner
  object: FIFA World Cup
}

This eliminates naive keyword heuristics.

2ï¸âƒ£ Structured Knowledge Grounding (Wikidata)

Resolves entity to QID

Maps relation â†’ Wikidata Property (PID)

Queries structured property values

Performs deterministic comparison

Example:

Property: P1346 (award received)
Value: FIFA World Cup 2022

If matched â†’ SUPPORTED
If mismatched â†’ CONTRADICTED

No LLM guessing required.

3ï¸âƒ£ NLI Semantic Verification

Model: facebook/bart-large-mnli

Used when structured verification is insufficient.

Compares:

Premise: Wikipedia summary
Hypothesis: Claim

Outputs:

ENTAILMENT

CONTRADICTION

NEUTRAL

4ï¸âƒ£ GPT Fallback

Model: gpt-4o-mini

Used only when:

No structured match

NLI confidence < threshold

Ensures:

Evidence-constrained reasoning

Strict JSON output

Calibrated confidence

5ï¸âƒ£ Hallucination Scoring Engine

Score computed from:

Contradiction ratio

Unknown ratio

Entropy

Variance

Confidence calibration

Produces:

hallucination_score âˆˆ [0, 1]

Risk interpretation:

0.0â€“0.3 â†’ Low Risk

0.3â€“0.6 â†’ Moderate Risk

0.6â€“1.0 â†’ High Risk

ğŸ“‚ Project Structure
app/
 â”œâ”€â”€ api/
 â”‚    â””â”€â”€ routes.py
 â”œâ”€â”€ core/
 â”‚    â”œâ”€â”€ orchestrator.py
 â”‚    â””â”€â”€ scorer.py
 â”œâ”€â”€ modules/
 â”‚    â”œâ”€â”€ triple_extractor.py
 â”‚    â”œâ”€â”€ property_mapper.py
 â”‚    â”œâ”€â”€ wikidata_property_retriever.py
 â”‚    â”œâ”€â”€ structured_verifier.py
 â”‚    â”œâ”€â”€ verifier.py
 â”‚    â”œâ”€â”€ llm_verifier.py
 â”‚    â”œâ”€â”€ entity_extractor.py
 â”‚    â””â”€â”€ claim_classifier.py
 â””â”€â”€ main.py
âš™ï¸ Installation
1. Install Dependencies
pip install fastapi uvicorn
pip install torch transformers accelerate sentencepiece
pip install requests
pip install spacy
python -m spacy download en_core_web_sm
2. Set OpenAI Key

Create .env file:

OPENAI_API_KEY=your_key_here
3. Run Backend
uvicorn app.main:app --reload --port 8001
ğŸ§ª Example Queries
Example 1
Argentina won the FIFA World Cup in 2022.

â†’ SUPPORTED
â†’ Hallucination â‰ˆ 0.02

Example 2
Argentina won the FIFA World Cup in 2018.

â†’ CONTRADICTED
â†’ Hallucination â‰ˆ 0.92

Example 3
Argentina has the strongest military in South America.

â†’ GPT fallback
â†’ Moderate risk

ğŸ” Research Characteristics

This system demonstrates:

Hybrid symbolic + neural reasoning

Knowledge graph grounding

Multi-layer verification

Transformer-based relation extraction

Deterministic + probabilistic fusion

Confidence-aware scoring

Modular research architecture

âš ï¸ Limitations

Property mapping is limited to predefined mappings

No multi-hop reasoning yet

No large-scale FEVER benchmarking

No entity disambiguation model

No property embedding alignment (future work)

ğŸš€ Future Work

Dynamic relation â†’ property embedding alignment

Multi-hop Wikidata reasoning

Entity linking via BLINK

Dense retrieval with FAISS

FEVER dataset evaluation

Calibration via temperature scaling

Ensemble verification voting

ğŸ¯ Target Use Cases

LLM output verification

Hallucination scoring pipelines

AI content moderation

Automated fact-checking prototypes

Research experimentation

ğŸ“œ License

MIT License

ğŸ‘¤ Author

Built as a research-oriented factual verification system combining structured knowledge graphs and transformer-based reasoning.
